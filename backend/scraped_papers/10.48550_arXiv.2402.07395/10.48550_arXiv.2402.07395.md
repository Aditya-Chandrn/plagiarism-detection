# Comparing The Willingness To Share For Human-Generated Vs. Ai-Generated Fake News

AMIRSIAVOSH BASHARDOUST, University of Lausanne, Faculty of Business and Economics, Switzerland STEFAN FEUERRIEGEL, Munich Center for Machine Learning (MCML) & LMU Munich, Germany YASH RAJ SHRESTHA, University of Lausanne, Faculty of Business and Economics, Switzerland Generative artificial intelligence (AI) presents large risks for society when it is used to create fake news. A crucial factor for fake news to go viral on social media is that users share such content. Here, we aim to shed light on the sharing behavior of users across human-generated vs. AI-generated fake news. Specifically, we study: (1) What is the perceived veracity of human-generated fake news vs. AI-generated fake news? (2) What is the user's willingness to share human-generated fake news vs. AI-generated fake news on social media? (3) What socio-economic characteristics let users fall for AI-generated fake news? To this end, we conducted a pre-registered, online experiment with  = 988 subjects and 20 fake news from the COVID-19 pandemic generated by GPT-4 vs.

humans. Our findings show that AI-generated fake news is perceived as less accurate than human-generated fake news, but both tend to be shared equally. Further, several socio-economic factors explain who falls for AI-generated fake news.

CCS Concepts: - Human-centered computing → Empirical studies in HCI; Social media; - **Applied computing** → *Sociology*.

Additional Key Words and Phrases: fake news, misinformation, generative AI, large language model, online experiment, survey

## 1 Introduction

```
Generative artificial intelligence (AI) refers to technologies that can create human-like content [9]. As such, generative
                                                                                                                          
AI enables many applications for the better of society, such as simplifying information search [7], supporting coding
                                                                                                                          
activities [42], and improving the accessibility for disabled people [12]. However, generative AI can also be used for
                                                                                                                          
harmful applications where it produces fake news. Although the problem of fake news and deepfakes is not new
                                                                                                                          
[40, 43, 45], recent advances in generative AI, such as GPT-4, have made it easy to produce fake news1that is highly
                                                                                                                          
realistic and thus hard to detect by humans [39]. To this end, AI-generated fake news presents immediate threats to
                                                                                                                          
society.
       
  The threat vectors of AI-generated fake news are characterized by three main factors [8], namely, scale, speed, and
                                                                                                                          
usability, as follows. (1) AI can be used to automate the mass production of fake news, likely leading to an uncontrollable
                                                                                                                          
influx of fake news which is difficult to detect and moderate. For example, AI tools can simply generate dozens of
                                                                                                                          
different fake news with different content around the same story. (2) AI can create fake news in seconds and can thus
                                                                                                                          
outpace traditional fact-checking. This is different from human-generated fake news, which requires time for writing
                                                                                                                          
and editing. (3) AI tools have become widely available, reducing the entry barriers for a broad user base to generate
                                                                                                                          
fake news without the need for specialized skills. As a consequence, it is crucial to prepare for the potential risks in an
                                                                                                                          
upcoming era of AI-generated fake news by understanding how users will respond to AI-generated fake news, so that
                                                                                                                          
effective, evidence-based mitigation strategies can be developed.
                                                                   
  Previous works have studied the characteristics of AI-generated content and what sets it aside from human-generated
                                                                                                                          
content. For example, one work compares human-generated vs. AI-generated content in terms of linguistic characteristics
                                                                                                                          
(e.g., use of slang, use of emotions) and semantic characteristics (e.g., level of detail, communication of uncertainties)
                                                                                                                          
[47]. Others have analyzed the heuristics with which humans discern human-generated vs. AI-generated content
                                                                                                                          
1We define fake news as "fabricated information that mimics news media content in form but not in organizational process or intent" [23], thus implying
                                                                                                                          
that it can overlap with other, related terms such as misinformation and disinformation.
                                                                       
                                                                                                                         1
                                                                                                                          

[19, 21, 39], finding that it is generally difficult for humans to infer where content is coming from [19]. Furthermore,
                                                                                                                               
Spitale et al. [39] evaluated whether users can predict whether true or false content was generated by humans or by
                                                                                                                               
an AI, finding that AI-generated content can disinform more effectively than compared to human-generated content.
                                                                                                                               
Hence, while the perception of false information was studied before, research studying the behavior of users interacting
                                                                                                                               
with false information is lacking. In particular, it is unclear whether users have a different willingness to share for
                                                                                                                               
human-generated vs. AI-generated fake news. Importantly, sharing activities for fake news are not necessarily related
                                                                                                                               
to the perception of fake news (e.g., even though some users disbelieve the content of fake news, they may share it
                                                                                                                               
nevertheless due to a lack of attention) [34].
                                               
   There are good reasons why to expect that users share human-generated vs. AI-generated fake news differently.
                                                                                                                               
(a) Human-generated fake news is shared more. Generating fake news by AI requires not only mimicking human-like
writing but also ideation of incorrect facts in the first place, which may be not sufficiently convincing. Hence, users do
                                                                                                                               
not believe the AI-generated fake news and thus do not share it. (b) AI-generated fake news is shared more. Generative
AI writes text in a highly structured and error-free manner, which makes it more convincing and thus increases the
                                                                                                                               
propensity of users to believe in AI-generated fake news and share it. (c) Human-generated and AI-generated fake news
are shared similarly. Since users cannot discern whether texts were generated by AI or humans, it is likely that online
users will fall for both human-generated and AI-generated fake news and thus share both at equal rates. Motivated by
                                                                                                                               
this, we study the following three research questions (RQs):

```

- **RQ1:** *What is the perceived veracity of human-generated vs. AI-generated fake news?*
- **RQ2:** *What is the willingness to share of human-generated fake news vs. AI-generated fake news?* - **RQ3:** Which socio-economic characteristics are determinants that explain which users fall for AI-generated fake news?

In this paper, we conducted a pre-registered, online experiment ( = 988 subjects) to compare how users respond to human-generated vs. AI-generated fake news.2In our experiment, we analyze two important variables characterizing online behavior around fake news, namely, perceived veracity and willingness to share. While perceived veracity has been studied earlier [39], we add by understanding the sharing behavior. Our experiment is based on a corpus of 20 fake news generated by GPT-4 and 20 by humans. We chose fake news from the COVID-19 pandemic due to the fact that wrong beliefs about it are still widespread and such wrong beliefs had large, negative effects for public health. We further perform regression analyses to identify various socio-economic determinants that characterize users likely to fall for fake news generated by AI. We find that AI-generated fake news is perceived as less accurate than human-generated fake news, but both tend to be shared equally.

## 2 Related Work 2.1 Fake News On Social Media

The phenomenon of fake news poses a great threat to the integrity of online information ecosystems and, more broadly, the functioning of modern societies. Among others, fake news has been used to meddle with elections [3], manipulate stock markets [6], and hamper public health [37, 38]. Furthermore, fake news circulated widely on social media during the COVID-19 pandemic and, as a result, caused several thousand deaths that could have been prevented [37, 38]. Still, the effective detection and mitigation of fake news present ongoing challenges [48].

2Code and data supporting our findings are available at public Git repository. The pre-registration protocol is available at https://osf.io/vhnju?view_

only=604564b14cf2471789b099a3164d109f 2 Extant research has examined the relationship between perceived veracity and subsequent sharing of traditional, human-generated fake news (e.g., [35]). Different elements in social media could distract the attention of users from judging the perceived veracity, which will affect their decision to share [35]. This has important implications for our paper: simply studying the perceived veracity of human-generated vs. AI-generated fake news does not inform how human-generated vs. AI-generated fake news is shared online. The latter is further relevant given that sharing is crucial for fake news (or any social media content more broadly) to go viral and eventually reach a large audience [11, 27].

## 2.2 Generative Ai For Content Creation 3

```
Recent advances in the capabilities of generative AI make it easy to produce content almost indistinguishable from that
                                                                                                                       
of humans [19, 37]. For instance, the Generative Pre-trained Transformer 3 (GPT-3) is able to produce a wide range
                                                                                                                       
of texts including writing lyrics for songs, completing documents, programming code, fictional stories, and cooking
                                                                                                                       
recipes [22]. As another example, large language models have reached a level of performance that ad content created by
                                                                                                                       
generative AI is perceived to be of higher quality compared to ad content produced by human experts [46].
                                                                                                            
  Research has also analyzed how users respond to AI-generated content [19], but outside of fake news. For example,
                                                                                                                       
research shows that people cannot distinguish between human-generated and AI-generated news stories [21]. Moreover,
                                                                                                                       
people who are more confident in detecting AI content made more errors in distinguishing both [29]. In another study,
                                                                                                                       
users rated news headlines as less accurate when they were told that the same headline was written by AI rather than
                                                                                                                       
by humans [24]. Others found that AI-generated content can be more persuasive than human-generated content, for
                                                                                                                       
example, in the context of propaganda [13] and letters to politicians [20]. Here, the underlying rationale is that AI lacks
                                                                                                                       
human sociological and psychological elements, such as individual desires and emotions, because of which outputs
                                                                                                                       
may be perceived to be less personal but more accurate. These features can also increase the perceived accuracy of
                                                                                                                       
AI-generated output more broadly [15, 16, 18]. However, the previous works focus on regular content (e.g., letters,
                                                                                                                       
dating profiles) and not fake news.
                                   
  Despite the potential harm of AI-generated fake news, there is, so far, little research exploring how users respond to
                                                                                                                       
AI-generated fake news, especially in comparison to human-generated fake news. One work has identified the linguistic
                                                                                                                       
differences between human-generated and AI-generated fake news [47]. However, the aforementioned analysis is
                                                                                                                       
focused on significant linguistic differences in expression patterns between human-generated and AI-generated fake
                                                                                                                       
news but not how users perceive the veracity of each. Even others focus on whether users can predict the source of fake
news [39]. However, research aimed at understanding the users' willingness to share fake news has remained unclear,
                                                                                                                       
even though is an essential precondition for fake news to go viral.
                                                                   
  Research gap: The perception of AI-generated content by users has been widely studied, but still, salient questions
about how users engage with AI-generated fake news are open (Table 1). To this end, we provide the first experimental
                                                                                                                       
study comparing the willingness to share of users across human-generated and AI-generated fake news.
                                                                                                         

```

## 3 Method

```
The experimental protocol was pre-registered at https://rb.gy/qu8bu. Ethics approval was granted by the Institutional
                                                                                                                     
Review Board at the University of (name anonymized for review).
                                                                 

```

Table 1. Key literature and how our study is different.

| Reference             | Scope                            | Content                        | Outcomes             |    |
|-----------------------|----------------------------------|--------------------------------|----------------------|----|
|                       |                                  | Perceived veracity             | Willingness to share |    |
| Pennycook et al. [34] | Human-generated                  | True vs. fake news             | ✔                    | ✔  |
| Spitale et al. [39]   | Human-generated vs. AI-generated | True vs. false Twitter/X posts | ✔                    | ✖  |
| Our study             | Human-generated vs. AI-generated | Fake news                      | ✔                    | ✔  |

## 3.1 Experimental Task

We conducted a within-subject online experiment, where subjects ( = 988) evaluated 20 items with fake news related to the COVID-19 pandemic. Out of the 20 items, 10 were human-generated and 10 were AI-generated fake news (details are in Sec. 3.3). The order in which the fake news items were displayed was randomized. Furthermore, subjects were neither informed that the content they were going to read was fake news nor that content was generated by AI.

Subjects were asked to respond to two questions for each fake news item:

(1) *Perceived veracity:* Subjects were asked to rate the perceived veracity of each item based on a 5-point Likert scale from 0 (*"strongly inaccurate"*) to 4 (*"strongly accurate"*), where 2 denotes a neutral assessment.

(2) *Willingness to share:* Subjects were asked to report their willingness to share the news item on their social media.

They could choose between *"not sharing"* (encoded as 0), *"sharing"* (encoded as 1), and *"don't know"* (excluded from analysis).

At this point, we highlight that the perceived veracity and the willingness to share measure different concepts. Importantly, the correlation coefficient between both is 0.162, which is referred to as weak correlation.

## 3.2 Subjects

Subjects were recruited from the U.S. via the online platform *Lucid* (https://luc.id/). A particular benefit of the Lucid platform is that it maintains a sample of subjects from the U.S. that is representative in terms of different demographic factors such as race, age, and gender. In our experiment, we intentionally focused on subjects from the U.S. to ensure that all subjects have a basic literacy in the English language. We did not compensate subjects directly; instead, subjects received a fixed compensation through *Lucid* for successful completion. The overall cost was EUR 1536. Given that the survey took only around 10 minutes to complete, the compensation of subjects is in alignment with the minimum federal wage [1].

We aimed for  = 1000 subjects. The choice was informed by earlier studies that analyze the perceived veracity and other determinants for human-generated fake news [33, 34]. Accordingly, 3610 subjects were initially recruited for our survey, out of which only 1185 of them passed the attention tests (described later) and completed the survey. Out of them, 193 subjects declared during the survey that they answered at least one question randomly or searched the Internet for the shown fake news, because of which we excluded the 193 subjects. We further excluded four subjects because they entered to be below the minimum age of 18 years, which was enforced by our Institutional Review Board.

Eventually, this led to 988 remaining subjects, which is close to our planned  = 1000 subjects.

Out of the 988 subjects, 626 were female, 342 were male, and 20 selected other genders. In our sample, 78.92%

identified themselves as white, and 11.25% as Black or African American. Our sample has a mean age of 47.53 years (SD

= 17.13). Out of our 988 subjects, 74.5% have been vaccinated against COVID-19. When considering political orientation, 4

| Content                                                                           | Label           |
|-----------------------------------------------------------------------------------|-----------------|
| "Federal Emergency Management Agency proposes martial law to contain virus."      | Human-generated |
| "Coconut oil's history in destroying viruses, including coronaviruses."           | Human-generated |
| "The second booster has 8 strains of HIV."                                        | Human-generated |
| "The coronavirus only spreads in cold weather or places with low humidity."       | AI-generated    |
| "COVID-19 vaccine causes infertility in women and men."                           | AI-generated    |
| "COVID-19 is less deadly than the flu and does not require any special measures." | AI-generated    |

Table 2. Examples of fake news in our corpus. Full list in the Online Supplements.
19.23% identified as left, 48.69% as moderate, 27.63% as right, and 4.45% as other. In terms of social orientation, 32.7%

identify as liberal, 38.6% as moderate, and the remaining 28.7% as conservative. Details about the questions are in Table 3.

## 3.3 Materials

```
We created a corpus of 10 human-generated and 10 AI-generated fake news around the COVID-19 pandemic. We chose
                                                                                                              
the COVID-19 pandemic for our study due to the fact there there is still a widespread belief in COVID-19 fake news
                                                                                                              
among society, with severe negative impact on public health [17]. Our process was as follows:
                                                                                        
    - Human-generated fake news. We followed previous research (e.g., [3, 23, 25, 26, 34, 44] and retrieved fake
      news from independent fact-checking websites, namely, PolitiFact (https://www.politifact.com) and Snopes
                                                                                                              
      (https://www.snopes.com). PolitiFact and Snopes are fact-checking websites that rate the accuracy of claims
                                                                                                              
      made by elected officials, candidates, political parties, and other groups. Their fact-checking process involves
                                                                                                              
      researching statements, interviewing experts, and then providing a detailed analysis of the claim. For our corpus,
                                                                                                              
      we collected items with a final rating of "pants on fire" or "false" at PolitiFact and "false" at Snopes. Besides these
      two sources, we also used COVID-19 fake news from a study by Pennycook et al. [34]. Overall, we used four fake
                                                                                                              
      news from Snopes, two from PolitiFact, and the rest from Pennycook et al. [34] research. The rationale behind
                                                                                                              
      our sampling strategy spanning different sources is to generate a more representative sample that also captures
                                                                                                              
      sufficient diversity among fake news items that proliferate online.3
    - AI-generated fake news. The sample of AI-generated fake news was created using GPT-4.4 Specifically, we asked
      GPT-4 to create fake news around the COVID-19 pandemic and to use a format similar to social media. Our
                                                                                                              
      corresponding prompt was as follows: "Write an incorrect statement about COVID-19 and put it in quotation marks."
      At the time when we accessed GPT-4, there were not safety measures that blocked our prompts. As a result,
                                                                                                              
      we were able to simply generate fabricated content with misleading information as outputs to our prompts.
                                                                                                              
      Currently, stricter safety measures are in place, and, hence, researcher would need to use alternative strategies to
                                                                                                              
      convince GPT-4 to output fabricated information, such as through impersonation techniques [39].
                                                                                                  
Table 2 shows examples of human-generated and AI-generated fake news. The complete list is in the Online Supplements.
                                                                                                              
Importantly, the human-generated vs. AI-generated fake news have a similar length to ensure comparable reading
                                                                                                             
effort.
     
3We performed robustness checks where we repeated the analysis for fake news from PolitiFact only or for fake news from Snopes only, yielding
                                                                                                              
consistent findings.
              
4We used the version from 14-03-2023, which we accessed via Bing Chat.
                                                      5

```

![5_image_0.png](5_image_0.png)

Fig. 1. The experiment's procedure flowchart.

## 3.4 Procedure

Our experimental procedure consists of seven steps. Figure 1 shows the flowchart of our experimental task.

```
   Step 1. Before the start of the experiment, subjects first visited a landing page. Subjects had to give informed consent
in order to proceed.
                      
   Step 2. Subjects were asked about their social media consumption. The first question is what kind of content they
share on social media (i.e., users could select multiple options out of "Political news", "Sport news", "Celebrity news",
"Science/technology news", "Business news" and "Others"). Then, we asked them what social media platform they use (i.e.,
users could select multiple options out of "Facebook", "Twitter", "Snapchat", "Instagram", "WhatsApp", "I do not use social
media", and "Others"). Finally, we asked them how much time (in hours) they spend, on average, on social media per
day.
    
   Furthermore, we asked subjects to attend an attention test (detailed in Table 4) in which we asked subjects to report
                                                                                                                                     
which news outlet they regularly check, yet the question concluded by stating that the question should be ignored and
                                                                                                                                     
a specific option should be chosen. If the attention test was failed, the survey was terminated immediately, and the
                                                                                                                                     
subject was excluded from our analysis to ensure that subjects read all texts thoroughly, which is analogous to other
                                                                                                                                     
online experiments [34].
                            
   Step 2 also included a six-item cognitive reflection test (CRT) [10]. Following Penneycook et al. [34], we used the
                                                                                                                                     
modified version of the original three CRT questions. This modification includes three additional non-numeric items
                                                                                                                                     
and has reworded questions. The CRT evaluates an individual's tendency to be intuitive [32, 41]. Each CRT question is
                                                                                                                                     
designed to trigger an intuitive yet incorrect answer. For instance, one of our CRT questions is: "A farmer had 15 sheep,
and all but 8 died. How many are left?" The intuitive answer one might think of is 7, but the correct answer is 8. The
ability to think critically is associated with answering CRT problems correctly. The items are aggregated, so that CRT
                                                                                                                                     
scores range between 0 (if all answers are incorrect) to 6 (if all responses are correct).
                                                                                                
   Step 3. We asked subjects three questions regarding the COVID-19 pandemic. The first question was how concerned
they were about the virus during the pandemic (as a percentage from 0% to 100%). The second question was how often
                                                                                                                                     
they checked the news about COVID-19 during the pandemic on a 5-point Likert scale from 1 ("never") to 5 ("very
often"). Finally, we asked them whether they had been vaccinated against COVID-19 ("yes" or "no"). Altogether, this
was done to later assess their prior disposition toward information around the COVID-19 pandemic [21, 34].
                                                                                                                           
                                                                  6

   Step 4. We first provided subjects with instructions about the experimental task, informing that they will face a series
of statements about COVID-19 and they should answer two questions for each statement. We then showed them 20
                                                                                                                                     
different fake news items that were either human-generated or AI-generated in random order. For each, we asked users
                                                                                                                                     
to assess (1) the accuracy of each item and (2) the willingness to share the item. We again emphasize that the subjects
                                                                                                                                     
were not aware that all items were fake news. During Step 4, we also monitored the time that users spent on each item
                                                                                                                                     
to later analyze it as a proxy of attention and cognitive involvement.
                                                                              
   We performed two more attention tests after the end of this step to check if subjects still are actively involved in
                                                                                                                                     
the experiment. However, unlike the first attention test, we did not terminate the experiment and they were able to
                                                                                                                                     
continue but excluded them from our analysis.5
   Step 5. After completion of the experimental task, we followed previous research [31] and asked subjects if, at any
point, they answered a question randomly or if they searched the Internet ("honesty tests"). We excluded those subjects
                                                                                                                                     
who answered this question with a "yes".6 We also conducted another attention test, which consisted of two questions:
                                                                                                                                      
(1) "5 + 4 = 11"; and (2) "The year 1820 came before the year 1910". For both questions, subjects had to rate the items
using 5-point Likert scale from 1 ("strongly disagree") to 5 ("strongly agree"). We excluded subjects who failed any of
these two questions.
                       
   Step 6. We further asked various questions about socio-demographics such as age, gender, years of education, ethnicity,
social and economic orientation (from 0 "strongly liberal" to 4 "strongly conservation"), religiosity ("religious person",
"not a religious person", and "atheist"), political orientation ("left", "moderate", "right", and "other"), fluency in English,
and annual income.
                      
   Step 7. We debriefed the subjects. In particular, all subjects were informed that all statement that they had seen
during the experiment were fake and that subjects should not rely on any of them.
                                                                                              
   The detailed questions and their possible answers are listed in Table 3.
                                                                                   

```

## 3.5 Statistical Analysis

```
To answer our research questions, we conducted the following statistical analyses:
                                                                                  
   (1) For RQ1, we performed hypothesis testing to compare the PerceivedVeracity across human-generated vs. AIgenerated fake news. Our analyses are at subject-rating level (i.e., one data point per fake news per subject;
                                                                                                                    
      20 × 988 = 19, 760 observations). We use the Mann-Whitney-U-test for testing our hypothesis.
   (2) For RQ2, we analogously performed hypothesis testing to compare the WillignessToShare of human-generated
      vs. AI-generated fake news. Our analysis is at subject-rating level. We removed "Don't know" answers, thereby
      yielding 17,676 observations. Here, we use the 
                                                      
                                                      2
                                                       -test.
                                                            
   (3) For RQ3, we performed a regression analysis to understand which determinants explain why users fall for
                                                                                                                    
      AI-generated (vs. human-generated) fake news. We constructed two linear mixed-effects regression models, one
                                                                                                                    
      for each dependent variable, with a subject-level random effect to account for between-subject heterogeneity.7
      In addition, we control for a number of behavior factors. We control for potential fatigue by including a variable
                                                                                                                    
      StatementSequence, which denotes the current number of already answered statements in the experiment.

```

5We further ran a robustness check, where the subjects were included, but arrived at conclusive findings.

6We repeated our analyses with the inclusion of these subjects and arrived at consistent conclusions.

7Our pre-registration originally stated that we account for between-subject heterogeneity through standard errors clustered at the subject level. However, we decided to follow recent research [26] and use random effects instead, which introduces a varying-intercept and thus allows for between-subject variation in the tendency to believe in fake news. Both analyses led to qualitatively similar conclusions.

7
Table 3. Detailed survey questions.

|                                                                                                                                                                                                                                                                                             |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | Table 3. Detailed survey questions. all incorrect (0) / ... / all correct (6)                                                                                                                                                                                                |
|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Variable name                                                                                                                                                                                                                                                                               | Question / measurement procedure                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | Options                                                                                                                                                                                                                                                                      |
| Dependent variables PerceivedVeracity                                                                                                                                                                                                                                                       | "How do you rate the accuracy of the statement below?"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | very inaccurate (0) / ... / very accurate (4)                                                                                                                                                                                                                                |
| WillingnessToShare                                                                                                                                                                                                                                                                          | "Will you share this statement on your social media"?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | yes (1) / no (0) / don't know (−1)                                                                                                                                                                                                                                           |
| Independent variables VaccinationStatus                                                                                                                                                                                                                                                     | "Have you been vaccinated against COVID-19?"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | yes (1) / no (0)                                                                                                                                                                                                                                                             |
| Concern                                                                                                                                                                                                                                                                                     | "How concerned were you about COVID-19 pandemic?"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | [0% (not concerned), 100% (extremely concerned)] never (0) / 1 / ... / very often (4)                                                                                                                                                                                        |
| NewsConsumption                                                                                                                                                                                                                                                                             | "How often did you proactively check COVID-19 pandemic news?"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                                                              |
| CRT                                                                                                                                                                                                                                                                                         | "The ages of Mark and Adam add up to 28 years total. Mark is 20 years older than Adam. How many years old is Adam?" "If it takes 10 seconds for 10 printers to print out 10 pages of paper, how many seconds will it take 50 printers to print out 50 pages?" "On a loaf of bread, there is a patch of mold. Every day, the patch doubles in size. If it takes 40 days for the patch to cover the entire loaf of bread, how many days would it take for the patch to cover half of the loaf of bread?" "If you're running a race and you pass the person in second place, what place are you in?" "A farmer had 15 sheep and all but 8 died. How many are left?" "Emily's father has three daughters. The first two are named April and May. What is the third daughter's name?" |                                                                                                                                                                                                                                                                              |
| Age                                                                                                                                                                                                                                                                                         | "What is your age? (in years)"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | -                                                                                                                                                                                                                                                                            |
| Gender∗                                                                                                                                                                                                                                                                                     | "What is your gender?"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | male (0) / female (1) / transgender female / transgender male / trans or non binary / not listed / prefer not to say                                                                                                                                                                                                                                                                              |
| Fluency                                                                                                                                                                                                                                                                                     | "Are you fluent in English?"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | yes (1) / no (0)                                                                                                                                                                                                                                                             |
| PoliticalOrientation                                                                                                                                                                                                                                                                        | "Which of the following best describes your political position?"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | left (0) / moderate (1) / right (2) / other (3)                                                                                                                                                                                                                              |
| SocialOrientation                                                                                                                                                                                                                                                                           | "On social issues I am:"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | strongly liberal (0) / 1 / ... / strongly conservative (4)                                                                                                                                                                                                                   |
| EconomicOrientation                                                                                                                                                                                                                                                                         | "On economic issues I am:"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | strongly liberal (0) / 1 / ... / strongly conservative (4)                                                                                                                                                                                                                   |
| EducationYears                                                                                                                                                                                                                                                                              | "How many years of formal education have you completed?"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | 0 / ... / 20]                                                                                                                                                                                                                                                                |
| Religiosity                                                                                                                                                                                                                                                                                 | "Would you say you are:"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | religious person (0) / not a religious person (1) / an atheist (2)                                                                                                                                                                                                           |
| Income†                                                                                                                                                                                                                                                                                     | "What is your household income (before taxes)?"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | [$10,000, $19,999] / . . . / [$150,000 or more] [$10,000, $29,999]: low (0) / [$30,000, $79,999]: middle (1) / ≥ $80,000: high (2)                                                                                                                                           |
| Ethnicity∗∗                                                                                                                                                                                                                                                                                 | "Which category best describes you?"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | White / Hispanic, Latino or Spanish origin / Black or African American / Asian / American Indian or Alaska Native / Middle Eastern or North African / Native Hawaiian or Other Pacific Islander / some other race, ethnicity or origin; encoded as White (0) / Non-White (1) |
| ∗ Only 20 subjects identified themselves as being different from male or female. Therefore, in our regression, we encoded Gender as a binary variable and remove other observations from the analysis due to the low statistical power. We later allow for between-subject variation beyond |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |                                                                                                                                                                                                                                                                              |

## The Regression Models Are As Follows:

PerceivedVeracity = 0 + 0 + 1 VaccinationStatus + 2 *Concern*
+ 3 NewsConsumption + 4 CRT 
+ 5 Age + 6 *Gender*
+ 7 PoliticalOrientation + 8 *EconomicOrientation*

+ 9 EducationYears + 10 Religiosity + 11 *Income*
+ 12 Ethnicity + 13 *StatementSequence* + ,
8

## (1)

Table 4. List of attention and honestly tests.

| Questions                                                                                                                                                        | Options                                              |
|------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------|
| Attention checks "When a big news story breaks, people often go online                                                                                           | New York Times / Yahoo! News /                       |
| to get up-to-the-minute details on what is going on. We also want                                                                                                | Huffington Post / NBC.com / CNN.com /                |
| know which websites people trust to get this information.                                                                                                        | USA Today / FoxNews.com / Others                     |
| to know if people are paying attention to following questions. Please ignore the question and select FoxNews.com and NBC.com as your two answers." " 5 + 4 = 11" | strongly disagree (0) / 1 / ... / strongly agree (4) |
| "The year 1820 came before the year 1910."                                                                                                                       | strongly disagree (0) / 1 / ... / strongly agree (4) |
| Honesty tests "Did you respond randomly at any point during the study? "                                                                                         | yes / no                                             |
| "Did you search the internet (via Google or otherwise) for help? "                                                                                               | yes / no                                             |

WillingnessToShare = 0 + 0 + 1 VaccinationStatus + 2 *Concern*
+ 3 NewsConsumption + 4 CRT 
+ 5 Age + 6 *Gender*
+ 7 PoliticalOrientation + 8 *EconomicOrientation*

+ 9 EducationYears + 10 Religiosity + 11 *Income*
+ 12 Ethnicity + 13 *StatementSequence* + ,
where *PerceivedVeracity* and *WillingnessToShare* are the dependent variables for -th observation of the -th subject, with intercepts 0, 0, subject-level random effect 0

, coefficients 1, . . .; 1*, . . .*, and an error term  .

In our implementation, we use the *lme4* package from R, which uses the restricted maximum likelihood (REML)
estimator. We use the REML estimator but also run robustness checks (see Sec 4.5) using logistic regression (for WillingnessToShare) and ordinal regression (for *PerceivedVeracity*). As in [34], ordinal and numerical variables are
-standardized to zero mean and one standard deviation to provide better interpretability. Categorical variables are coded numerically (see Table 3). We did not add *SocialOrientation* in the regression as it is highly collinear with *EconomicOrientation*.

8

## 3.6 Ethical Considerations

We respect the privacy and agency of all people potentially impacted by this work and take specific steps to protect their privacy. To this end, all data were collected in an anonymous form, and any personally identifiable features were removed. All steps during data collection and analyses followed standards for ethical research [36]. All subjects gave informed consent, including for data publication.

We further acknowledge that our analysis exposed people to fake news. The specific experimental design was approved by the Institutional Review Board at the University of (anonymized for peer-review). To mitigate potential risks for subjects, we debriefed subjects regarding the aim of the survey and disclosed that all statements were false at the end of the survey and emphasized that they should not rely on them. Moreover, we intentionally chose the COVID-19 pandemic as infection rates have plummeted and the immediate health risk has decreased over time (as opposed to other fake news narratives that currently circulate online).

8We repeated the analysis by exchanging the EconomicOrientation by *SocialOrientation* which leads to conclusive findings.

9

$\left(2\right)$. 

Finally, we acknowledge that our findings could also be used by malicious actors running fake news campaigns.

However, there are immediate threats from AI-generated fake news campaigns for public trust [8], and we thus follow earlier calls for research [8] that ask for more evidence as a first step to understand risk vectors and design effective policy responses.

## 4 Results 4.1 Comparing Perceived Veracity For Human-Generated Vs. Ai-Generated Fake News (Rq1)

Our first research question is: "What is the perceived veracity of human-generated vs. AI-generated fake news?".

Previously, Spitale et al. [39] has conducted an experimental study demonstrating that people find false Twitter/X

posts created by GPT-3 more accurate than human-written posts. Inspired by these results, we intend to replicate this investigation in our own experimental setting using fake news (instead of user-generated Twitter/X posts).

Figure 2 shows the distribution of subjects' perceived veracity scores grouped by whether the fake news is humangenerated or AI-generated. We performed a Mann-Whitney-U-test to examine whether human-generated and AIgenerated fake news perceived veracity scores have identical distributions. The result confirmed that the distributions of the perceived veracity scores are not identical across human-generated and AI-generated fake news ( < 0.001).

The average perceived veracity scores of AI-generated fake news is 19.67% smaller than that of human-generated fake news. Therefore, subjects perceived human-generated fake news more accurately compared to AI-generated fake news. Consequently, we find a negative answer to our RQ1: AI-generated fake news is considered less accurate than human-generated fake news.

![9_image_0.png](9_image_0.png)

Fig. 2. Distribution of perceived veracity scores across human-generated vs. AI-generated fake news.
In addition, we repeat the above analysis using a binary variable of whether users find fake news accurate or not.

```
Formally, the binary variable amounts to 1 if a subject rates fake news as "somehow accurate" or "very accurate", and 0
otherwise. Overall, 98.9% of the users have at least once failed to correctly identify AI-generated fake news as such.
                                                                                                                        
We then calculated the relative number of how often users fail to identify fake news as such and thus respond with
                                                                                                                        
"somehow accurate" and "very accurate". We use Welch's -test to check if there is a significant difference in the average
rate of incorrect perceived veracity assessments for human-generated and AI-generated fake news. Figure 3 shows
                                                                                                                        
                                                           10

that human-generated fake news has a significantly ( < 0.001) higher rate of incorrect perceived veracity assessment.
                                                                                                                    
On average, subjects failed 49.6% when rating human-generated fake news, while they failed 36.3% when rating AIgenerated fake news. As a robustness check, we also employed a linear mixed-effects regression with subject-level
                                                                                                                   
random effects, which corroborated our findings.
                                                
  Our findings for fake news are contrary to the findings for user-generated social media content from Spitale et
                                                                                                                   
al. [39]. In their study, AI could disinform subjects better than humans, implying that the perceived veracity of fake
                                                                                                                   
social media posts generated by AI is higher than for fake social media posts generated by humans. However, our
findings suggest that the perceived veracity of AI-generated fake news is smaller than for human-generated fake news.
The different findings with regard to the perceived veracity could be rooted in the type of generated content. In their
                                                                                                                   
study, Spitale et al. [39] used Twitter/X posts, while we use fake news. Hence, it is likely that user-generated social
                                                                                                                   
media content such as Twitter/X posts may be written and generally perceived differently than news. For example,
                                                                                                                    
social media posts written by humans may have grammatical errors, which users can use as cues to detect that such
                                                                                                                   
posts are fake, while news are generally written without grammatical errors regardless of whether the news originates
                                                                                                                   
from humans or an AI).
                       

![10_image_0.png](10_image_0.png)

```

Fig. 3. The average rate of incorrect perceived veracity assessments across subjects. Welch's -test shows that human-generated fake news has a significantly ( < 0.001) higher rate of incorrect perceived veracity assessments compared to AI-generated fake news.

## 4.2 Comparing Willingness To Share For Human-Generated Vs. Ai-Generated Fake News (Rq2)

To address RQ2, we present our findings regarding the subjects' willingness to share for fake news. Specifically, we

```
compare the willingness of subject to share human-generated fake news versus their willingness to share AI-generated
                                                                                                                    
fake news. This comparison aims to discern any significant differences in the sharing behavior elicited by the source
                                                                                                                    
of the fake news. Overall, a substantial portion of the subjects would share human-generated fake news (11.59%) and
                                                                                                                    
AI-generated fake news (11.94%). We used a 
                                           
                                           2
                                            -test to examine whether there is a significant difference between whether
                                                                                                                    
subjects share human-generated compared to AI-generated fake news. We found that there is no statistically significant
                                                                                                                    
                                                         11

```

difference between the willingness to share human-generated and AI-generated fake news ( = 0.69). Therefore, we find

![11_image_0.png](11_image_0.png)

a negative answer to RQ2: there is no difference in willingness to share between human-generated and AI-generated fake news. As a robustness check, we also performed linear mixed-effects regression with subject-level random effects, finding consistent results that there is no statistically significant difference with respect to willingness to share at common significance thresholds.

Fig. 4. Subjects' fake news willingness to share grouped by source of generation. The 2
-test shows that there is no significant difference between AI-generated vs. human-generated fake news.

## 4.3 **Socio-Economic Characteristics As Determinants Explaining Which Users Fall For Ai-Generated Fake** News (Rq3)

Perceived veracity: We now analyze which socio-economic characteristics are determinants that explain the perceived veracity of users. For this, we use a linear mixed-effects regression with *PerceivedVeracity* as the dependent variable and socio-economic variables as our independent variables. We developed mixed-effects linear regression models with subject-level random effects. We run separate regressions for human-generated fake news and AI-generated fake news, so that we can identify differences in how both are received by subjects. Our findings are shown in Figure 5.

NewsConsumption and *Concern* are not associated with perceived veracity in any of the models ( > 0.05). CRT and VaccinationStatus are are positive statistically significant and thus are determinants of the perceived veracity, while CRT has a negative association with the perceived veracity of the fake news. For a one standard deviation increase in CRT, we expect that the perceived veracity reduces by a 7% standard deviation. Regarding *Religiosity*, we observe that atheists (in reference to religious subjects) do not have any statistical significance in perceived veracity. However, non-religious subjects (in reference to religious subjects) have a negative association with perceived veracity ( < 0.01).

For *EconomicOrientation*, we observe that the different categories do not have a statistically significant ( > 0.05)
effect. When it comes to *PoliticalOrientation*, the results show that politically moderate and right subjects perceived the 12 veracity of fake news higher (as compared to politically left). We can see that the association is slightly different for human-generated as compared to AI-generated fake news. Further, Age is also statistically significant ( < 0.001): a one standard deviation increase in age, we will correspond to a 15.6% standard deviations decrease in perceived veracity.

Regarding *Gender*, females, with reference to males, perceived the veracity of the fake news as lower, and it is also statistically significant ( < 0.001). This association exists both for human-generated and AI-generated fake news.

![12_image_0.png](12_image_0.png)

Fig. 5. Estimates explaining the perceived veracity through various socio-economic variables using linear mixed-effects regression models. All models include subject-level random effects.
Willingness to share: We now analyze which socio-economic variables are associated with *WillingnessToShare* (see Figure 6). In contrast to *PerceivedVeracity*, our findings show that *NewsConsumption* ( < 0.001) and *Income* ( < 0.01)
are significantly positively associated with *WillingnessToShare*. However, the effect size is small. Interestingly, a larger values for *Consumption* and *Concern* correspond to a larger *WillingnessToShare*. Furthermore, CRT, PoliticalOrientation, Religiosity, EducationYears, *Ethnicity*, and Age follow the same pattern as for *PerceivedVeracity*. Although *Gender* is still statistically significant ( < 0.001), the estimate for human-generated fake news and AI-generated fake news is very close, unlike as for *PerceivedVeracity* as the dependent variable.

## 4.4 Extensions

Users may vary in their attention. To control for that, we consider the subjects' average response time as a potential determinant in how accurately users can assess the veracity of fake news. To this end, we calculated the average response time at the subject-level for human-generated and AI-generated fake news. To reduce the influence of outliers on our estimate, we capped the upper bound of the average response time to 90th percentile. Figure 7 shows the average response time for each subject for both human-generated and AI-generated fake news statements. A Welch -test 13

![13_image_0.png](13_image_0.png)

Fig. 6. Estimates explaining the willingness to share through various socio-economic variables using linear mixed-effects regression models. All models include subject-level random effects.
confirms that subjects responded to AI-generated fake news significantly faster than human-generated ( < 0.001).

However, the difference in the means is comparatively small (0.604 seconds).9

## 4.5 Robustness Checks

```
We followed our pre-registration and conducted several robustness checks to validate our results. (1) We calculated the
                                                                                                                       
variance inflation factor (VIF) for all regression models. All VIFs are below the critical threshold of five [2], implying
                                                                                                                      
that there is no concern due to multicollinearity. (2) One of our dependent variables is ordinal (i.e., PerceivedVeracity),
and the other is binary (i.e., WillingnessToShare). We nevertheless chose ordinary least squares for reasons of better
interpretability. We additionally repeated our analysis with a logistic regression and an ordinal regression, respectively,
                                                                                                                       
yielding similar findings. (3) We excluded subjects who either failed the second set of attention tests or indicated that
                                                                                                                       
they selected answers randomly or searched the Internet during the experiment. When including the subjects instead,
                                                                                                                       
we arrived at consistent findings. In sum, all of the robustness checks supported our findings.
                                                                                              

```

## 5 Discussion 5.1 Relevance

The phenomenon of AI-generated fake news has emerged as a critical concern. While the concept of fabricating

information, such as deepfakes [5], is not new, the advent of large language models and other generative AI techniques 9We also repeated the above regression analysis using the response time as an addition control for between-subject heterogeneity in attention but we arrived at conclusive findings.

14
Fig. 7. Subjects' average response time in seconds grouped by human-generated vs. AI-generated fake news. The upper bound of the

![14_image_0.png](14_image_0.png)

data is capped at 90th percentile to avoid the influence of outliers. A Welch's -test shows a significant difference in the means of human-generated vs. AI-generated fakes news ( ≃ 0). However, as the plot shows, the effect size is fairly small.

```
make it easy to create fake news at an unprecedented scale [8, 28]. In particular, AI-generated fake news could make it
                                                                                                                      
difficult for online users to discern what is true and what is false, thus amplifying the threat vectors from fake news
                                                                                                                      
with detrimental consequences for public trust. Hence, research such as ours is needed to explore the vulnerabilities of
                                                                                                                      
individuals to AI-generated fake news.
                                       
  Large language models possess the capability to produce fake news that is difficult for humans to detect. In particular,
                                                                                                                      
we have demonstrated empirically that 98.9% of the subjects have at least once fallen for the AI-generated fake news in
                                                                                                                      
our sample. This finding can be attributed to the ability of state-of-the-art large language models to mimic the style
                                                                                                                      
and content of reputable sources. As such, many cues that humans have previously used to detect fake content are no
                                                                                                                      
longer valid. For example, checking the grammatical accuracy within texts is no longer sufficient since large language
                                                                                                                      
models can write virtually error-free textual content.
                                                     
  Previous research has already analyzed whether humans can discern AI-generated content from human-generated
                                                                                                                      
content, such as for dating profiles, hostility profiles, or letters [19, 21]). Here, humans are often unable to accurately
                                                                                                                      
detect AI-generated content. However, the aforementioned results are based on regular, truthful content, whereas
                                                                                                                      
the phenomenon of fake news introduces additional complexity in that a false statement around which to write fake
                                                                                                                      
news must also be fabricated. The latter presents the focus of our study. Previously, the perception of users in terms of
perceived veracity of human-generated vs. AI-generated fake news was analyze [39], while our novelty is to analyze the
                                                                                                                      
behavior in terms of willingness to share. The latter is crucial to understand, as sharing is an important precondition
for fake news to go viral and thus reach a large audience. Importantly, as we show in our study, perceived veracity and
                                                                                                                      
sharing behavior are only weakly correlated and thus capture different activities of online behavior.
                                                                                                    
                                                          15

```

## 5.2 Implications Of Findings

```
Our paper has three main findings. First, even though our results highlight that AI-generated fake news is perceived
                                                                                                                    
to be less accurate than human-generated fake news, a large percentage of subjects fall for AI-generated fake news.
                                                                                                                    
Second, we do not find a statistically significant difference in the willingness of subjects to share human-generated vs.
                                                                                                                    
AI-generated fake news despite the relatively large sample size, highlighting the risk that fabricated content may go
                                                                                                                    
easily viral. Third, we find that (some) socio-economic factors explain whether users fall for AI-generated fake such as
                                                                                                                    
age and political orientation. In our setting with fake news from the COVID-19 pandemic, subjects who disclose to be
                                                                                                                    
conservatives were especially prone to fall for AI-generated fake news. This finding indicates that people from different
                                                                                                                    
socio-economic backgrounds could fall for AI-generated fake news also in different ways.
                                                                                        
  As implications, we see three potential directions that are valuable to counter risks due to AI-generated fake news.
                                                                                                                    
(1) Novel cues are needed that users can leverage to discern true from fake content, yet such cues must not be easily
                                                                                                                    
gamed by AI technologies. Specific examples are digital watermarks or community-based fact-checks (e.g., Community
                                                                                                                    
Notes at Twitter/X). (2) Our findings highlight the importance of making people aware of the risks of AI-generated
                                                                                                                    
fake news, implying that tailored media literacy trainings are needed. For example, one could train online users in
                                                                                                                    
how generative AI works so that they check the perceived veracity of online content more carefully. (3) Online users
                                                                                                                    
may fall for AI-generated fake news without noticing, because of which regulatory approaches may be needed [14].
                                                                                                                    
Notwithstanding, as our regression results show, the vulnerabilities to AI-generated fake news can also be pronounced
                                                                                                                    
for certain marginalized groups (e.g., with a lower cognitive reflection test score, young age).
                                                                                            

```

## 5.3 Limitations

As with other works, our work has several limitations that open up avenues for future research. First, our study is

```
based on fake news about COVID-19. While the COVID-19 pandemic had large negative impacts on public health, the
                                                                                                               
results may be different for other contexts (e.g., armed conflicts) and other forms of disinformation (e.g., propaganda,
                                                                                                               
rumors, hoaxes). Hence, future research may replicate our analyses in such varied contexts. Second, we only measure
                                                                                                               
the willingness to share and not actual decisions to share. However, this is consistent with prior research [34], and,
                                                                                                               
on top of that, it would be highly unethical to ask subjects to actually share fake news in their social media account.
                                                                                                               
Third, our results are based on GPT-4, a large language model with state-of-the-art performance [30]. However, the
                                                                                                               
research landscape around generative AI technologies is quickly evolving, and new models with better capabilities will
                                                                                                               
be developed over time (including generative AI technologies for multi-modal outputs). Here, it will be interesting to
                                                                                                               
repeat our research with new models and other modalities such as fake images and videos.
                                                                                     

```

## 5.4 Conclusion

Generative AI can give rise to harmful applications when it is used to generate fake news. Yet, it is unclear how online users interact with AI-generated fake news. We thus provided a large-scale analysis with experimental evidence, finding that humans are likely to share AI-generated fake news but that the sharing propensity is similar for human-generated and AI-generated fake news.

```
REFERENCES
[1] 2023. Minimum wage | USAGov. https://www.usa.gov/minimum-wage
                                       
[2] Michael Olusegun Akinwande, Hussaini Garba Dikko, and Agboola Samson. 2015. Variance Inflation Factor: As a Condition for the Inclusion of
                                                                           
  Suppressor Variable(s) in Regression Analysis. Open Journal of Statistics 5, 7 (Dec. 2015), 754–767. https://doi.org/10.4236/ojs.2015.57075 Number: 7
                                     16

     Publisher: Scientific Research Publishing.
                                               
 [3] Hunt Allcott and Matthew Gentzkow. 2017. Social media and fake news in the 2016 election. Journal of economic perspectives 31, 2 (2017), 211–236.
     Publisher: American Economic Association 2014 Broadway, Suite 305, Nashville, TN 37203-2418.
                                                                                                       
 [4] Jesse Bennett. 2021. Improving the way we categorize family income. https://www.pewresearch.org/decoded/2021/06/04/improving-the-way-wecategorize-family-income/
                                
 [5] Joanna M. Burkhardt. 2017. Combating fake news in the digital age. ALA TechSource, Chicago, IL. OCLC: 1012609982.
 [6] Jonathan Clarke, Hailiang Chen, Ding Du, and Yu Jeffrey Hu. 2020. Fake news, investor attention, and market reaction. Information Systems Research
     32, 1 (2020), 35–52. Publisher: INFORMS.
                                               
 [7] Yogesh K. Dwivedi, Nir Kshetri, Laurie Hughes, Emma Louise Slade, Anand Jeyaraj, Arpan Kumar Kar, Abdullah M. Baabdullah, Alex Koohang,
                                                                                                                                                         
     Vishnupriya Raghavan, Manju Ahuja, Hanaa Albanna, Mousa Ahmad Albashrawi, Adil S. Al-Busaidi, Janarthanan Balakrishnan, Yves Barlette,
                                                                                                                                                         
     Sriparna Basu, Indranil Bose, Laurence Brooks, Dimitrios Buhalis, Lemuria Carter, Soumyadeb Chowdhury, Tom Crick, Scott W. Cunningham,
                                                                                                                                                         
     Gareth H. Davies, Robert M. Davison, Rahul Dé, Denis Dennehy, Yanqing Duan, Rameshwar Dubey, Rohita Dwivedi, John S. Edwards, Carlos Flavián,
                                                                                                                                                         
     Robin Gauld, Varun Grover, Mei-Chih Hu, Marijn Janssen, Paul Jones, Iris Junglas, Sangeeta Khorana, Sascha Kraus, Kai R. Larsen, Paul Latreille, Sven
                                                                                                                                                         
     Laumer, F. Tegwen Malik, Abbas Mardani, Marcello Mariani, Sunil Mithas, Emmanuel Mogaji, Jeretta Horn Nord, Siobhan O'Connor, Fevzi Okumus,
                                                                                                                                                         
     Margherita Pagani, Neeraj Pandey, Savvas Papagiannidis, Ilias O. Pappas, Nishith Pathak, Jan Pries-Heje, Ramakrishnan Raman, Nripendra P. Rana,
                                                                                                                                                         
     Sven-Volker Rehm, Samuel Ribeiro-Navarrete, Alexander Richter, Frantz Rowe, Suprateek Sarker, Bernd Carsten Stahl, Manoj Kumar Tiwari, Wil
                                                                                                                                                         
     van der Aalst, Viswanath Venkatesh, Giampaolo Viglia, Michael Wade, Paul Walton, Jochen Wirtz, and Ryan Wright. 2023. Opinion Paper: "So what
                                                                                                                                                         
     if ChatGPT wrote it?" Multidisciplinary perspectives on opportunities, challenges and implications of generative conversational AI for research,
                                                                                                                                                         
     practice and policy. International Journal of Information Management 71 (Aug. 2023), 102642. https://doi.org/10.1016/j.ijinfomgt.2023.102642
 [8] Stefan Feuerriegel, Renée DiResta, Josh A. Goldstein, Srijan Kumar, Philipp Lorenz-Spreen, Michael Tomz, and Nicolas Pröllochs. 2023. Research can
                                                                                                                                                         
     help to tackle AI-generated disinformation. Nature Human Behaviour 7, 11 (Nov. 2023), 1818–1821. https://doi.org/10.1038/s41562-023-01726-2
     Number: 11 Publisher: Nature Publishing Group.
                                                       
 [9] Stefan Feuerriegel, Jochen Hartmann, Christian Janiesch, and Patrick Zschech. 2024. Generative AI. Business & Information Systems Engineering
     (2024).
            
[10] Shane Frederick. 2005. Cognitive Reflection and Decision Making. Journal of Economic Perspectives 19, 4 (Dec. 2005), 25–42. https://doi.org/10.1257/
     089533005775196732
                          
[11] Dominique Geissler, Dominik Bär, Nicolas Pröllochs, and Stefan Feuerriegel. 2023. Russian propaganda on social media during the 2022 invasion of
                                                                                                                                                         
     Ukraine. EPJ Data Science 12, 1 (Dec. 2023), 35. https://doi.org/10.1140/epjds/s13688-023-00414-5 Number: 1 Publisher: Springer Berlin Heidelberg.
[12] Deepak Giri and Erin Brady. 2023. Exploring outlooks towards generative AI-based assistive technologies for people with Autism. https:
                                                                                                                                                         
     //doi.org/10.48550/arXiv.2305.09815 arXiv:2305.09815 [cs].
                                                                
[13] Josh A. Goldstein, Jason Chao, Shelby Grossman, Alex Stamos, and Michael Tomz. 2023. Can AI Write Persuasive Propaganda? https://doi.org/10.
                                                                                                                                                          
     31235/osf.io/fp87b
                        
[14] Josh A. Goldstein, Girish Sastry, Micah Musser, Renee DiResta, Matthew Gentzel, and Katerina Sedova. 2023. Generative Language Models and
                                                                                                                                                         
     Automated Influence Operations: Emerging Threats and Potential Mitigations. https://doi.org/10.48550/arXiv.2301.04246 arXiv:2301.04246 [cs].
                                                                                                                                                        
[15] Heather M. Gray, Kurt Gray, and Daniel M. Wegner. 2007. Dimensions of Mind Perception. Science 315, 5812 (Feb. 2007), 619–619. https:
     //doi.org/10.1126/science.1134475
                                       
[16] Kurt Gray and Daniel M. Wegner. 2012. Feeling robots and human zombies: Mind perception and the uncanny valley. Cognition 125, 1 (Oct. 2012),
     125–130. https://doi.org/10.1016/j.cognition.2012.06.007
                                                               
[17] Tiffany Hsu. 2022. As Covid-19 Continues to Spread, So Does Misinformation About It. The New York Times (Dec. 2022). https://www.nytimes.
     com/2022/12/28/technology/covid-misinformation-online.html
                                                                     
[18] Arthur S. Jago and Kristin Laurin. 2022. Assumptions About Algorithms' Capacity for Discrimination. Personality and Social Psychology Bulletin 48,
     4 (April 2022), 582–595. https://doi.org/10.1177/01461672211016187
                                                                          
[19] Maurice Jakesch, Jeffrey T. Hancock, and Mor Naaman. 2023. Human heuristics for AI-generated language are flawed. Proceedings of the National
     Academy of Sciences 120, 11 (March 2023), e2208839120. https://doi.org/10.1073/pnas.2208839120 Publisher: Proceedings of the National Academy
     of Sciences.
                 
[20] Sarah Kreps and Douglas L. Kriner. 2023. The potential impact of emerging technologies on democratic representation: Evidence from a field
                                                                                                                                                         
     experiment. New Media & Society (March 2023), 146144482311605. https://doi.org/10.1177/14614448231160526
[21] Sarah Kreps, R. Miles McCain, and Miles Brundage. 2022. All the News That's Fit to Fabricate: AI-Generated Text as a Tool of Media Misinformation.
                                                                                                                                                          
     Journal of Experimental Political Science 9, 1 (March 2022), 104–117. https://doi.org/10.1017/XPS.2020.37 Publisher: Cambridge University Press.
[22] Nils C. Köbis and Luca Mossink. 2020. Artificial intelligence versus Maya Angelou: Experimental evidence that people cannot differentiate
                                                                                                                                                         
     AI-generated from human-written poetry. Comput. Hum. Behav. 114 (2020), 106553. https://api.semanticscholar.org/CorpusID:221562332
[23] David M. J. Lazer, Matthew A. Baum, Yochai Benkler, Adam J. Berinsky, Kelly M. Greenhill, Filippo Menczer, Miriam J. Metzger, Brendan Nyhan,
                                                                                                                                                         
     Gordon Pennycook, David Rothschild, Michael Schudson, Steven A. Sloman, Cass R. Sunstein, Emily A. Thorson, Duncan J. Watts, and Jonathan L.
                                                                                                                                                          
     Zittrain. 2018. The science of fake news. Science 359, 6380 (March 2018), 1094–1096. https://doi.org/10.1126/science.aao2998
[24] Chiara Longoni, Andrey Fradkin, Luca Cian, and Gordon Pennycook. 2022. News from Generative Artificial Intelligence Is Believed Less. In 2022
     ACM Conference on Fairness, Accountability, and Transparency (FAccT '22). Association for Computing Machinery, New York, NY, USA, 97–106.
                                                                           17

     https://doi.org/10.1145/3531146.3533077
                                            
[25] Bernhard Lutz, Marc Adam, Stefan Feuerriegel, Nicolas Pröllochs, and Dirk Neumann. 2024. Which linguistic cues make people fall for fake news?
                                                                                                                                                   

```

A comparison of cognitive and affective processing. ACM. arXiv:2312.03751 [cs].

[26] Bernhard Lutz, Marc T. P. Adam, Stefan Feuerriegel, Nicolas Pröllochs, and Dirk Neumann. 2023. Affective Information Processing of Fake News:

```
     Evidence from NeuroIS. European Journal of Information Systems 0, 0 (June 2023), 1–20. https://doi.org/10.1080/0960085X.2023.2224973 Publisher:
    Taylor & Francis _eprint: https://doi.org/10.1080/0960085X.2023.2224973.
                                                                        
[27] Abdurahman Maarouf, Nicolas Pröllochs, and Stefan Feuerriegel. 2023. The Virality of Hate Speech on Social Media. https://doi.org/10.48550/arXiv.
                                                                                                                                            
     2210.13770 arXiv:2210.13770 [cs].
                                    
[28] Bradley D. Menz, Natansh D. Modi, Michael J. Sorich, and Ashley M. Hopkins. 2024. Health Disinformation Use Case Highlighting the Urgent Need
                                                                                                                                            
     for Artificial Intelligence Vigilance: Weapons of Mass Disinformation. JAMA Internal Medicine 184, 1 (Jan. 2024), 92–96. https://doi.org/10.1001/
     jamainternmed.2023.5947
                            
[29] Elizabeth J. Miller, Ben A. Steward, Zak Witkower, Clare A. M. Sutherland, Eva G. Krumhuber, and Amy Dawel. 2023. AI Hyperrealism: Why AI Faces
                                                                                                                                            
    Are Perceived as More Real Than Human Ones. Psychological Science 34, 12 (Dec. 2023), 1390–1403. https://doi.org/10.1177/09567976231207095
     Publisher: SAGE Publications Inc.
                                    
[30] OpenAI. 2023. GPT-4 Technical Report. https://doi.org/10.48550/arXiv.2303.08774 arXiv:2303.08774 [cs].
                                                                                                      
[31] Gordon Pennycook, Tyrone D. Cannon, and David G. Rand. 2018. Prior exposure increases perceived accuracy of fake news. Journal of Experimental
     Psychology: General 147, 12 (2018), 1865–1880. https://doi.org/10.1037/xge0000465 Place: US Publisher: American Psychological Association.
[32] Gordon Pennycook, James Allan Cheyne, Derek J. Koehler, and Jonathan A. Fugelsang. 2016. Is the cognitive reflection test a measure of both
                                                                                                                                            
     reflection and intuition? Behavior Research Methods 48, 1 (March 2016), 341–348. https://doi.org/10.3758/s13428-015-0576-1
[33] Gordon Pennycook, Ziv Epstein, Mohsen Mosleh, Antonio Arechar, Dean Eckles, and David Rand. 2020. Understanding and Reducing the Spread of
                                                                                                                                            
     Misinformation Online. ACR North American Advances NA-48 (2020). https://www.acrwebsite.org/volumes/2662454/volumes/v48/NA-48
[34] Gordon Pennycook, Jonathon McPhetres, Yunhao Zhang, Jackson G. Lu, and David G. Rand. 2020. Fighting COVID-19 Misinformation on
                                                                                                                                            
     Social Media: Experimental Evidence for a Scalable Accuracy-Nudge Intervention. Psychological Science 31, 7 (July 2020), 770–780. https:
    //doi.org/10.1177/0956797620939054 Publisher: SAGE Publications Inc.
                                                                      
[35] Gordon Pennycook and David G. Rand. 2021. The Psychology of Fake News. Trends in Cognitive Sciences 25, 5 (May 2021), 388–402. https:
    //doi.org/10.1016/j.tics.2021.02.007
                                    
[36] Caitlin M. Rivers and Bryan L. Lewis. 2014. Ethical research standards in a world of big data. Technical Report 3:38. F1000Research. https:
    //f1000research.com/articles/3-38 Type: article.
                                                
[37] Sebastian W Schuetz, Tracy Ann Sykes, and Viswanath Venkatesh. 2021. Combating COVID-19 fake news on social media through fact checking:
                                                                                                                                            
     antecedents and consequences. European Journal of Information Systems 30, 4 (2021), 376–388. Publisher: Taylor & Francis.
[38] Anuragini Shirish, Shirish C Srivastava, and Shalini Chandra. 2021. Impact of mobile connectivity and freedom on fake news propensity during the
                                                                                                                                            
     COVID-19 pandemic: a cross-country empirical examination. European Journal of Information Systems 30, 3 (2021), 322–341. Publisher: Taylor &
     Francis.
            
[39] Giovanni Spitale, Nikola Biller-Andorno, and Federico Germani. 2023. AI model GPT-3 (dis)informs us better than humans. Science Advances 9, 26
    (June 2023), eadh1850. https://doi.org/10.1126/sciadv.adh1850 Publisher: American Association for the Advancement of Science.
                                                                                                                            
[40] Rob Toews. 2023. Deepfakes Are Going To Wreak Havoc On Society. We Are Not Prepared. https://www.forbes.com/sites/robtoews/2020/05/25/
                                                                                                                                            
     deepfakes-are-going-to-wreak-havoc-on-society-we-are-not-prepared/ Section: AI.
                                                                                    
[41] Maggie E. Toplak, Richard F. West, and Keith E. Stanovich. 2011. The Cognitive Reflection Test as a predictor of performance on heuristics-and-biases
                                                                                                                                            
     tasks. Memory & Cognition 39, 7 (Oct. 2011), 1275–1289. https://doi.org/10.3758/s13421-011-0104-1
[42] Priyan Vaithilingam, Tianyi Zhang, and Elena L. Glassman. 2022. Expectation vs. Experience: Evaluating the Usability of Code Generation Tools
                                                                                                                                            
     Powered by Large Language Models. In Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems (CHI EA '22).
    Association for Computing Machinery, New York, NY, USA, 1–7. https://doi.org/10.1145/3491101.3519665
                                                                                                       
[43] Emily van der Nagel. 2020. Verifying images: deepfakes, control, and consent. Porn Studies 7, 4 (Oct. 2020), 424–429. https://doi.org/10.1080/
     23268743.2020.1741434 Publisher: Routledge.
                                              
[44] Soroush Vosoughi, Deb Roy, and Sinan Aral. 2018. The spread of true and false news online. Science 359, 6380 (2018), 1146–1151. https:
    //doi.org/10.1126/science.aap9559 _eprint: https://www.science.org/doi/pdf/10.1126/science.aap9559.
                                                                                                  
[45] Mika Westerlund. 2019. The Emergence of Deepfake Technology: A Review. Technology Innovation Management Review 9, 11 (Jan. 2019), 39–52.
     https://doi.org/10.22215/timreview/1282
                                          
[46] Yunhao Zhang and Renee Gosline. 2023. Human Favoritism, Not AI Aversion: People's Perceptions (and Bias) Toward Generative AI, Human
                                                                                                                                            
     Experts, and Human-GAI Collaboration in Persuasive Content Generation. https://doi.org/10.2139/ssrn.4453958
                                                                                                             
[47] Jiawei Zhou, Yixuan Zhang, Qianni Luo, Andrea G Parker, and Munmun De Choudhury. 2023. Synthetic Lies: Understanding AI-Generated
                                                                                                                                            
     Misinformation and Evaluating Algorithmic and Human Solutions. In Proceedings of the 2023 CHI Conference on Human Factors in Computing
     Systems. ACM, Hamburg Germany, 1–20. https://doi.org/10.1145/3544548.3581318
[48] Xinyi Zhou and Reza Zafarani. 2021. A Survey of Fake News: Fundamental Theories, Detection Methods, and Opportunities. Comput. Surveys 53, 5
    (Sept. 2021), 1–40. https://doi.org/10.1145/3395046
                                                    

```
